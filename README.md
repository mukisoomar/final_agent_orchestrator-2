# ğŸ§  Agent Orchestrator Framework

This is a modular framework for orchestrating agents that process input files through a multi-step pipeline. Each agent performs a transformation on the input, and the output is passed downstream in a configurable flow.

---

## ğŸ“ Folder Structure

```
agent_orchestrator_refactored/
â”œâ”€â”€ agents/              # Contains BaseAgent and a generic Agent implementation
â”œâ”€â”€ orchestrator/        # Orchestration logic to run agent pipelines
â”œâ”€â”€ utils/               # Logging setup
â”œâ”€â”€ config/              # Flow configuration (flow.json)
â”œâ”€â”€ prompts/             # System prompt per agent (e.g., agent_generate_brd.txt)
â”œâ”€â”€ repository/          # Input files to be processed
â”œâ”€â”€ output/              # Output folder structured by input file name
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ requirements.txt
```

---

## âš™ï¸ How It Works

1. **Drop input files** (e.g., `.tal` files) into the `repository/` folder.
2. **Define agent flow** in `config/flow.json`.
3. **Provide a system prompt** in `prompts/{agent_name}.txt` for each agent.
4. Run the orchestrator:

```bash
python main.py
```

Each file in `repository/` is fed to the first agent. Outputs are written in `output/{filename}/` for every agent in the chain.

---

## ğŸ” Sample `flow.json`

```json
{
  "agent_document_tal_code": ["agent_generate_brd"],
  "agent_generate_brd": ["agent_generate_java_tech_specs"],
  "agent_generate_java_tech_specs": ["agent_generate_java_code"],
  "agent_generate_java_code": []
}
```

This defines a sequence: `agent_document_tal_code â†’ agent_generate_brd â†’ agent_generate_java_tech_specs â†’ agent_generate_java_code`.

---

## ğŸ§© Prompt Example

`prompts/agent_generate_brd.txt`:

```
You are a business analyst generating a BRD from the TAL code provided.
```

---

## ğŸ” Optional: OpenAI Integration

To integrate real LLM APIs:

- Replace the simulated logic in `Agent.run()` with a call to `openai.ChatCompletion.create()`.
- Load your API key using `.env` or environment variables.

---

## âœ… Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Output Example

For a file named `sample1.tal`:

```
output/
â””â”€â”€ sample1/
    â”œâ”€â”€ agent_document_tal_code.txt
    â”œâ”€â”€ agent_generate_brd.txt
    â”œâ”€â”€ agent_generate_java_tech_specs.txt
    â””â”€â”€ agent_generate_java_code.txt
```

---

## ğŸ™Œ Extend This

- Add custom agent subclasses for advanced logic.
- Use LangChain, Gemini, or other LLMs.
- Add a Streamlit interface to visualize results.

---

## ğŸ” API Key Setup

Create a `.env` file in the root directory with your OpenAI key:

```
OPENAI_API_KEY=your_openai_api_key_here
```

Ensure you have [an OpenAI account](https://platform.openai.com/) and billing enabled.

You can also switch to Gemini API integration if needed by customizing the `Agent.run()` method.

---

## âš™ï¸ Model Configuration

### Global defaults

You can configure global model parameters in:

```
config/default_model_config.json
```

Example:

```json
{
  "model": "gpt-4",
  "temperature": 0.3,
  "max_tokens": 1024
}
```

### Per-agent overrides

Customize specific agents in:

```
config/agent_config.json
```

Example:

```json
{
  "agent_generate_brd": {
    "temperature": 0.2
  },
  "agent_generate_java_code": {
    "max_tokens": 2048
  }
}
```

Each agent will merge its config with the defaults and call OpenAI using its own settings.

---

## ğŸ§  OpenAI & Gemini Support

You can run the agent framework with either:

- `OpenAI` (ChatGPT)
- `Gemini` (Google)

### ğŸ”§ To configure:

In `config/default_model_config.json` or per-agent in `agent_config.json`, specify:

```json
{
  "provider": "openai", // or "gemini"
  "model": "gpt-4", // or "gemini-pro"
  "temperature": 0.3,
  "max_tokens": 1024
}
```

---

## ğŸ” API Key Setup

Add a `.env` file with one or both of:

```
OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
```

---

## ğŸ“Š Token Usage Logging

Each agent logs:

- Prompt token count
- Completion token count
- Total token usage

Log files are written to:

```
logs/agent.log
```

This helps monitor cost and efficiency of each LLM call.

---

## ğŸ§  How Prompts Work in Agents

Each agent constructs a full conversation prompt before sending it to the LLM. This is based on three parts:

### 1. ğŸ§¾ System Prompt

- File: `prompts/{agent_name}/system.txt`
- This sets the behavior of the LLM (e.g., "You are an expert Java developer...")

### 2. ğŸ§© User Prompt Template

- File: `prompts/{agent_name}/user_template.txt`
- This is a template containing placeholders for context, such as:

```txt
Use the following business requirements and technical specs to generate Java code:

Business Requirements:
{{agent_generate_brd}}

Technical Specs:
{{agent_generate_java_tech_specs}}
```

- These placeholders are replaced with actual outputs from earlier agents by name.

### 3. ğŸ§  Assistant Context Messages

- Each previous agent's output is also added as a separate message in the conversation:

```json
{
  "role": "assistant",
  "content": "[Context from agent_generate_brd]:\n<output text here>"
}
```

This gives the LLM the full conversational history including prior content â€” which helps it make better decisions.

---

## ğŸ—ƒ Example Message Construction

Letâ€™s say `agent_generate_java_code` is being executed. Here's how its message list will look:

```json
[
  {
    "role": "system",
    "content": "You are an expert Java developer tasked with converting specs into Java classes."
  },
  {
    "role": "assistant",
    "content": "[Context from agent_generate_brd]:\n...BRD Output..."
  },
  {
    "role": "assistant",
    "content": "[Context from agent_generate_java_tech_specs]:\n...Tech Specs Output..."
  },
  {
    "role": "user",
    "content": "Use the following inputs to generate code:\nBRD:\n{{agent_generate_brd}}\nSpecs:\n{{agent_generate_java_tech_specs}}"
  }
]
```

Both the assistant context and user prompt contain the same values â€” but structured differently to align with how LLMs use context and instruction.

---

## ğŸ Final Output

After executing, the LLM response is saved to:

```
output/<input_filename>/<agent_name>.txt
```

Each agent receives its input from the original file or a previous output, and all context is propagated automatically.


---

## ğŸ”Œ Extending Agents for Custom Logic

You can create custom agents by subclassing the base `Agent` class. This allows you to run additional logic like calling external APIs, validating results, or preprocessing input data.

### ğŸ§ª Example: Agent with External API Call

File: `agents/agent_call_external_service.py`

```python
import requests
from agents.agent import Agent

class AgentCallExternalService(Agent):
    def run(self, input_path, output_folder, previous_outputs=None):
        self.logger.info("Calling external service...")

        try:
            response = requests.get("https://jsonplaceholder.typicode.com/todos/1")
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            self.logger.error(f"External call failed: {e}")
            raise RuntimeError("Could not fetch external data")

        # Add external data as context
        if previous_outputs is None:
            previous_outputs = {}
        previous_outputs["external_service"] = f"Todo fetched: {data['title']}"

        # Run the standard LLM agent logic
        super().run(input_path, output_folder, previous_outputs)
```

### ğŸ§© Configuration

#### `config/flow.json`
```json
{
  "agent_call_external_service": []
}
```

#### `config/agent_config.json`
```json
{
  "agent_call_external_service": {
    "output_file": "external_output.txt"
  }
}
```

### ğŸ§  Use Case

- Pre-fetch knowledge from APIs
- Query databases or search indexes
- Combine external reasoning with LLM output
